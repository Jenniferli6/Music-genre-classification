{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11347438,"sourceType":"datasetVersion","datasetId":7099628}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport librosa\nimport librosa.display\n\n# to play the audio files\nfrom IPython.display import Audio\nfrom sklearn.model_selection import train_test_split\n\n\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import LearningRateScheduler\nimport tensorflow as tf\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout\nfrom collections.abc import Iterable\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, BatchNormalization\nfrom keras import layers, callbacks\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.regularizers import l2\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T21:32:58.077671Z","iopub.execute_input":"2025-04-27T21:32:58.077959Z","iopub.status.idle":"2025-04-27T21:33:13.934070Z","shell.execute_reply.started":"2025-04-27T21:32:58.077933Z","shell.execute_reply":"2025-04-27T21:33:13.933336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T21:33:13.935366Z","iopub.execute_input":"2025-04-27T21:33:13.935836Z","iopub.status.idle":"2025-04-27T21:33:13.939408Z","shell.execute_reply.started":"2025-04-27T21:33:13.935815Z","shell.execute_reply":"2025-04-27T21:33:13.938733Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Reading Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/musicgenreclassification/features_30_sec.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T21:33:13.940043Z","iopub.execute_input":"2025-04-27T21:33:13.940229Z","iopub.status.idle":"2025-04-27T21:33:13.991731Z","shell.execute_reply.started":"2025-04-27T21:33:13.940213Z","shell.execute_reply":"2025-04-27T21:33:13.991259Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### EDA","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:34:08.746654Z","iopub.execute_input":"2025-04-21T05:34:08.746844Z","iopub.status.idle":"2025-04-21T05:34:08.790899Z","shell.execute_reply.started":"2025-04-21T05:34:08.746824Z","shell.execute_reply":"2025-04-21T05:34:08.790304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:34:08.791637Z","iopub.execute_input":"2025-04-21T05:34:08.792015Z","iopub.status.idle":"2025-04-21T05:34:08.796236Z","shell.execute_reply.started":"2025-04-21T05:34:08.791998Z","shell.execute_reply":"2025-04-21T05:34:08.795658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nselected_columns = [col for col in df.columns if \"mfcc\" in col.lower()]\nsubset_corr = df[selected_columns].corr()\n\nplt.figure(figsize=(28, 24))\nsns.heatmap(subset_corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.title(\"Correlation Heatmap of Selected MFCC Features\", fontsize=16)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:34:08.796851Z","iopub.execute_input":"2025-04-21T05:34:08.797045Z","iopub.status.idle":"2025-04-21T05:34:11.904612Z","shell.execute_reply.started":"2025-04-21T05:34:08.797030Z","shell.execute_reply":"2025-04-21T05:34:11.903629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:34:11.905424Z","iopub.execute_input":"2025-04-21T05:34:11.905641Z","iopub.status.idle":"2025-04-21T05:34:11.910459Z","shell.execute_reply.started":"2025-04-21T05:34:11.905623Z","shell.execute_reply":"2025-04-21T05:34:11.909838Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature Engineering / Train Test split","metadata":{}},{"cell_type":"code","source":"if df['label'].dtype == 'object':\n    le = LabelEncoder()\n    df['label'] = le.fit_transform(df['label'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:34:11.911325Z","iopub.execute_input":"2025-04-21T05:34:11.911625Z","iopub.status.idle":"2025-04-21T05:34:11.942654Z","shell.execute_reply.started":"2025-04-21T05:34:11.911606Z","shell.execute_reply":"2025-04-21T05:34:11.942038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop(columns = ['filename'], inplace = True)\n\ny = df['label']\nX = df.drop(columns = ['label'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:34:11.945057Z","iopub.execute_input":"2025-04-21T05:34:11.945277Z","iopub.status.idle":"2025-04-21T05:34:11.960536Z","shell.execute_reply.started":"2025-04-21T05:34:11.945261Z","shell.execute_reply":"2025-04-21T05:34:11.959995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\nprint(\"Training set shape:\", X_train.shape)\nprint(\"Testing set shape:\", X_test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:34:11.961326Z","iopub.execute_input":"2025-04-21T05:34:11.962321Z","iopub.status.idle":"2025-04-21T05:34:11.978476Z","shell.execute_reply.started":"2025-04-21T05:34:11.962297Z","shell.execute_reply":"2025-04-21T05:34:11.977963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:34:11.979018Z","iopub.execute_input":"2025-04-21T05:34:11.979196Z","iopub.status.idle":"2025-04-21T05:34:12.012149Z","shell.execute_reply.started":"2025-04-21T05:34:11.979175Z","shell.execute_reply":"2025-04-21T05:34:12.011202Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:34:12.012996Z","iopub.execute_input":"2025-04-21T05:34:12.013176Z","iopub.status.idle":"2025-04-21T05:34:12.016457Z","shell.execute_reply.started":"2025-04-21T05:34:12.013161Z","shell.execute_reply":"2025-04-21T05:34:12.015917Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Training","metadata":{}},{"cell_type":"markdown","source":"#### KNN","metadata":{}},{"cell_type":"code","source":"knn_model = KNeighborsClassifier(n_neighbors=3)\nknn_model.fit(X_train_scaled, y_train)\n\n# Predict on the test set\ny_pred = knn_model.predict(X_test_scaled)\n\n# Calculate accuracy on the test set\nknn_accuracy = accuracy_score(y_test, y_pred)\nprint(\"KNN Model Test Accuracy:\", knn_accuracy)\n\n# Save the test accuracy into the results dictionary with key 'KNN'\nresults['KNN'] = knn_accuracy\n\n# Optionally, print the dictionary to verify\nprint(\"Results:\", results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:34:12.017134Z","iopub.execute_input":"2025-04-21T05:34:12.017345Z","iopub.status.idle":"2025-04-21T05:34:12.081420Z","shell.execute_reply.started":"2025-04-21T05:34:12.017325Z","shell.execute_reply":"2025-04-21T05:34:12.080145Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### RandomForest","metadata":{}},{"cell_type":"code","source":"rf_model = RandomForestClassifier(random_state=42)\n\n# Fit the model on the training data\nrf_model.fit(X_train_scaled, y_train)\n\n# Predict the labels for the test data\ny_pred = rf_model.predict(X_test_scaled)\n\n# Calculate the test accuracy\nrf_accuracy = accuracy_score(y_test, y_pred)\nprint(\"Random Forest Test Accuracy:\", rf_accuracy)\n\n# Save the test accuracy into the results dictionary under the key 'RandomForest'\nresults['RandomForest'] = rf_accuracy\n\n# Optionally, print the results dictionary to verify the stored result\nprint(\"Results:\", results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:34:12.082045Z","iopub.execute_input":"2025-04-21T05:34:12.082462Z","iopub.status.idle":"2025-04-21T05:34:12.509524Z","shell.execute_reply.started":"2025-04-21T05:34:12.082441Z","shell.execute_reply":"2025-04-21T05:34:12.508945Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### XgBoost","metadata":{}},{"cell_type":"code","source":"xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n\n# Train the model on the training data\nxgb_model.fit(X_train_scaled, y_train)\n\n# Predict on the test set\ny_pred = xgb_model.predict(X_test_scaled)\n\n# Calculate accuracy on the test set\nxgb_accuracy = accuracy_score(y_test, y_pred)\nprint(\"XGBoost Model Test Accuracy:\", xgb_accuracy)\n\n# Save the test accuracy into the results dictionary under the key 'XGBoost'\nresults['XGBoost'] = xgb_accuracy\n\n# Print out the results dictionary to verify\nprint(\"Results:\", results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:34:12.510213Z","iopub.execute_input":"2025-04-21T05:34:12.510435Z","iopub.status.idle":"2025-04-21T05:34:13.729308Z","shell.execute_reply.started":"2025-04-21T05:34:12.510412Z","shell.execute_reply":"2025-04-21T05:34:13.728761Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Neural Network","metadata":{}},{"cell_type":"code","source":"num_classes  = y_train.nunique()\n\nnn_model_deep = Sequential([\n    tf.keras.Input(shape=(X_train_scaled.shape[1],)),\n    Dense(1028, activation='relu'),\n    Dropout(0.4),\n    BatchNormalization(),\n    \n    Dense(1028, activation='relu'),\n    Dropout(0.4),\n    BatchNormalization(),\n    \n    Dense(256, activation='relu'),\n    Dropout(0.4),\n    BatchNormalization(),\n    \n    Dense(128, activation='relu'),\n    Dropout(0.4),\n    BatchNormalization(),\n    \n    Dense(32, activation='relu'),\n    Dropout(0.4),\n    BatchNormalization(),\n    \n    Dense(num_classes, activation='softmax')\n])\n\n\n# Compile the model\nnn_model_deep.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Set up EarlyStopping to monitor validation loss with increased patience to allow more training time\nearly_stop_deep = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\nnn_model_deep.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:34:13.730038Z","iopub.execute_input":"2025-04-21T05:34:13.731592Z","iopub.status.idle":"2025-04-21T05:34:16.067936Z","shell.execute_reply.started":"2025-04-21T05:34:13.731570Z","shell.execute_reply":"2025-04-21T05:34:16.067113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_filepath_nn = 'best_nn_model.keras'\nmodel_checkpoint_callback_nn = ModelCheckpoint(\n    filepath=checkpoint_filepath_nn,\n    save_best_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:34:16.068698Z","iopub.execute_input":"2025-04-21T05:34:16.069231Z","iopub.status.idle":"2025-04-21T05:34:16.072471Z","shell.execute_reply.started":"2025-04-21T05:34:16.069206Z","shell.execute_reply":"2025-04-21T05:34:16.071814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history_deep = nn_model_deep.fit(X_train_scaled, y_train,\n                                 validation_split=0.2,\n                                 epochs=512,\n                                 batch_size=128,\n                                callbacks=[model_checkpoint_callback_nn],\n                                 verbose=1)\n\n# Evaluate the deeper model on the test set\ntest_loss_deep, test_accuracy_deep = nn_model_deep.evaluate(X_test_scaled, y_test, verbose=0)\n\nresults['NeuralNetwork'] = test_accuracy_deep\n\n\nnn_model_deep.save(\"best_neural_network_model.h5\")\n\n\nprint(\"Deeper Neural Network Model Accuracy:\", test_accuracy_deep)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:34:16.073232Z","iopub.execute_input":"2025-04-21T05:34:16.073406Z","iopub.status.idle":"2025-04-21T05:34:57.739464Z","shell.execute_reply.started":"2025-04-21T05:34:16.073392Z","shell.execute_reply":"2025-04-21T05:34:57.738843Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### LSTM","metadata":{}},{"cell_type":"code","source":"\nlstm_model = Sequential([\n    LSTM(256, return_sequences=False, input_shape=(40,1), recurrent_dropout=0.3,\n         kernel_regularizer=l2(0.001)),\n    Dropout(0.3),\n    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n    Dropout(0.3),\n    Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n    Dropout(0.3),\n    Dense(10, activation='softmax')\n])\n\nlstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nlstm_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:34:57.740689Z","iopub.execute_input":"2025-04-21T05:34:57.740920Z","iopub.status.idle":"2025-04-21T05:34:57.909668Z","shell.execute_reply.started":"2025-04-21T05:34:57.740901Z","shell.execute_reply":"2025-04-21T05:34:57.909134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_filepath = 'best_model.keras'\nmodel_checkpoint_callback = ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_best_only=True,\n    monitor='val_accuracy',\n    mode='max',\n    verbose=1)\n\n\ndef scheduler(epoch, lr):\n    if epoch < 10:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)\n\nlr_scheduler_callback = LearningRateScheduler(scheduler)\n\nhistory = lstm_model.fit(\n    X_train_scaled, y_train,\n    batch_size=128,\n    validation_split=0.2,\n    epochs=500,\n    callbacks=[model_checkpoint_callback],\n    verbose=1\n)\ntest_loss_deep_lstm, test_accuracy_deep_lstm = lstm_model.evaluate(X_test_scaled, y_test, verbose=0)\n\nresults['LSTM'] = test_accuracy_deep_lstm\n\n\n# nn_model.save(\"best_lstm.h5\")\n\n\nprint(\"Deeper Neural Network Model Accuracy:\", test_accuracy_deep_lstm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:34:57.910369Z","iopub.execute_input":"2025-04-21T05:34:57.910609Z","iopub.status.idle":"2025-04-21T05:37:22.300936Z","shell.execute_reply.started":"2025-04-21T05:34:57.910583Z","shell.execute_reply":"2025-04-21T05:37:22.300316Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Final Results","metadata":{}},{"cell_type":"code","source":"results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T05:37:22.301740Z","iopub.execute_input":"2025-04-21T05:37:22.302286Z","iopub.status.idle":"2025-04-21T05:37:22.306745Z","shell.execute_reply.started":"2025-04-21T05:37:22.302260Z","shell.execute_reply":"2025-04-21T05:37:22.306225Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Experimentation 1","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# 1) Force Python, NumPy, and TF hashes & RNGs to be deterministic\nos.environ['PYTHONHASHSEED']     = '42'\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\nrandom.seed(42)\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# 2) Prepare experiment container\nexper1 = {}\ntest_sizes = [0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25, 0.3, 0.35]\n\nfor test_size in test_sizes:\n    # 3) Split with fixed seed\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y,\n        test_size=test_size,\n        random_state=42,  # ensures same split every time\n        shuffle=True\n    )\n\n    # 4) Standardize\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled  = scaler.transform(X_test)\n\n    # 5) Clone & compile model after seeding\n    tf.random.set_seed(42)  # reseed before weight init\n    nn_model = tf.keras.models.clone_model(nn_model_deep)\n    nn_model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    # 6) Setup checkpoint\n    checkpoint_filepath = f'best_model_{int(test_size*100)}.keras'\n    ckpt_cb = ModelCheckpoint(\n        filepath=checkpoint_filepath,\n        save_best_only=True,\n        monitor='val_accuracy',\n        mode='max',\n        verbose=1\n    )\n\n    # 7) Train (deterministically)\n    history = nn_model.fit(\n        X_train_scaled, y_train,\n        validation_split=0.2,\n        epochs=512,\n        batch_size=128,\n        callbacks=[ckpt_cb],\n        verbose=0\n    )\n\n    # 8) Evaluate & record\n    loss, acc = nn_model.evaluate(X_test_scaled, y_test, verbose=0)\n    exper1[f\"test_size_{int(test_size*100)}\"] = {\n        \"Test_Accuracy\": acc,\n        \"Model\": nn_model\n    }\n\n    print(f\"Test size={test_size:.3f} → Accuracy={acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T06:13:45.404929Z","iopub.execute_input":"2025-04-21T06:13:45.405181Z","iopub.status.idle":"2025-04-21T06:20:09.166564Z","shell.execute_reply.started":"2025-04-21T06:13:45.405163Z","shell.execute_reply":"2025-04-21T06:20:09.165774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_sizes = [0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25, 0.3, 0.35]\n\n# Match test_sizes to dictionary keys\nprint(\"\\n📊 Test Accuracies by Test Set Size:\\n\" + \"-\"*45)\nfor i, test_size in enumerate(test_sizes):\n    key = f\"test_size_{int(test_size*100):02d}\"\n    acc = exper1[key]['Test_Accuracy']\n    print(f\"Test size = {test_size:<6} →  Accuracy: {acc:.4f}\")\n\n# Find the best performing model\nbest_key = max(exper1, key=lambda k: exper1[k]['Test_Accuracy'])\nbest_acc = exper1[best_key]['Test_Accuracy']\nbest_model = exper1[best_key]['Model']\n\n# Extract the corresponding test size from the list\nbest_index = list(exper1.keys()).index(best_key)\nbest_test_size = test_sizes[best_index]\n\nprint(\"\\n🏆 Best Performing Split:\")\nprint(f\"Test size = {best_test_size} with Accuracy: {best_acc:.4f}\")\n\n# Save best model\nfilename = f\"best_model_testsize_{str(best_test_size).replace('.', '_')}.h5\"\nbest_model.save(filename)\nprint(f\"\\n💾 Best model saved as: {filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T06:25:35.058311Z","iopub.execute_input":"2025-04-21T06:25:35.058905Z","iopub.status.idle":"2025-04-21T06:25:35.158738Z","shell.execute_reply.started":"2025-04-21T06:25:35.058883Z","shell.execute_reply":"2025-04-21T06:25:35.157877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Extract data for plotting\ntest_sizes = [0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25, 0.3, 0.35]\naccuracies = [exper1[f\"test_size_{int(ts*100)}\"][\"Test_Accuracy\"] for ts in test_sizes]\n\n# Plot the test accuracy line\nplt.figure(figsize=(10, 6))\nplt.plot(test_sizes, accuracies, color='black', linewidth=2.5, label='Model A')\n\n# Optional: add baseline (e.g., 0.5)\nbaseline = 0.5\nplt.axhline(y=baseline, color='lightgray', linestyle='--', linewidth=1.5)\nplt.text(test_sizes[-1] + 0.005, baseline + 0.005, \"Baseline\", color='gray')\n\n# Labels for each axis\nplt.xlabel(\"Test Size\", fontsize=12)\nplt.ylabel(\"Test Accuracy\", fontsize=12)\n\n# Annotate the highest point\nbest_idx = int(np.argmax(accuracies))\nplt.text(test_sizes[best_idx], accuracies[best_idx] + 0.02,\n         f\"{accuracies[best_idx]*100:.1f}%\", color='black', fontsize=12)\n\n# Add model label at end of line\nplt.text(test_sizes[-1] + 0.005, accuracies[-1], \"Model A\", fontsize=12, color='black')\n\n# Grid and title\nplt.grid(alpha=0.3)\nplt.title(\"Test Accuracy by Test Set Size\", fontsize=14)\n\n# Plot style\nplt.ylim(0, 1.05)\nplt.xlim(min(test_sizes) - 0.01, max(test_sizes) + 0.03)\nplt.xticks(test_sizes)\nplt.tight_layout()\nplt.savefig('ModelSplitPerformance.png')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T06:25:37.145527Z","iopub.execute_input":"2025-04-21T06:25:37.146204Z","iopub.status.idle":"2025-04-21T06:25:37.432230Z","shell.execute_reply.started":"2025-04-21T06:25:37.146179Z","shell.execute_reply":"2025-04-21T06:25:37.431557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_model = exper1[best_key][\"Model\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.125, random_state=42\n)\n\n# 2. Scale test data using the same method as before\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n# 4. Predict class probabilities and convert to labels\ny_pred_probs = best_model.predict(X_test_scaled)\ny_pred = np.argmax(y_pred_probs, axis=1)\n\n# 5. Compute accuracy\ntest_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\n✅ Test Accuracy (confirmed): {test_accuracy:.4f}\")\n\n# 6. Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\ngenre_labels = le.inverse_transform(np.arange(10))  # Map class numbers to genre names\n\n# 7. Plot confusion matrix\nfig, ax = plt.subplots(figsize=(10, 8))\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=genre_labels)\ndisp.plot(cmap='Blues', xticks_rotation=45, ax=ax)\n\nplt.title(\"Confusion Matrix - Feature Based Classifier\", fontsize=14)\nplt.xlabel(\"Predicted Genre\", fontsize=12)\nplt.ylabel(\"True Genre\", fontsize=12)\nplt.tight_layout()\nplt.savefig('Confusion')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T06:35:14.890563Z","iopub.execute_input":"2025-04-21T06:35:14.891277Z","iopub.status.idle":"2025-04-21T06:35:15.623138Z","shell.execute_reply.started":"2025-04-21T06:35:14.891253Z","shell.execute_reply":"2025-04-21T06:35:15.622313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T06:39:47.008071Z","iopub.execute_input":"2025-04-21T06:39:47.008387Z","iopub.status.idle":"2025-04-21T06:39:47.013360Z","shell.execute_reply.started":"2025-04-21T06:39:47.008355Z","shell.execute_reply":"2025-04-21T06:39:47.012746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature Extraction Automation","metadata":{}},{"cell_type":"code","source":"class AudioFeature:\n    import os \n    import numpy as np\n    import librosa\n\n    def __init__(self, path):\n        self.path = path\n\n        if os.path.exists(self.path):\n            self.y, self.sr = librosa.load(self.path)\n\n        else:\n            raise Exception(f\"Path not found - {self.path}\")\n\n        self.load_audio_file()\n\n\n    def get_dataframe(self):\n        \"\"\"\n        Return all extracted features as a single-row pandas DataFrame\n        with columns matching the feature names.\n        \"\"\"\n        features = self.get_all_params()\n        \n        df = pd.DataFrame([features])\n        \n        return df\n\n    def get_all_params(self):\n\n        self.get_length()\n        self.get_zero_crossings()\n        self.get_tempo()\n        self.get_centroids()\n        self.get_spectral_rolloff()\n        self.get_mel_frequencies()\n        self.get_chroma()\n        self.get_rms()\n        self.get_spectral_bandwith()\n        self.get_harmony()\n        self.get_perceptr()\n        \n        result = {\n            \"length\" : self.length,\n            \"chroma_stft_mean\" : self.chroma_stft_mean,\n            \"chroma_stft_var\" : self.chroma_stft_var,\n            \"rms_mean\" : self.rms_mean,\n            \"rms_var\" : self.rms_var,\n            \"spectral_centroid_mean\" : self.spectral_centriod_mean,\n            \"spectral_centroid_var\" : self.spectral_centriod_var,\n            \"spectral_bandwith_mean\" : self.spectral_bandwith_mean,\n            \"spectral_bandwith_var\" : self.spectral_bandwith_var,\n            \"rolloff_mean\" : self.rolloff_mean,\n            \"rolloff_var\" : self.rolloff_var,\n            \"zero_crossing_rate_mean\": self.zero_crossings_rate_mean,\n            \"zero_crossing_rate_var\" : self.zero_crossings_rate_var,\n            \"harmony_mean\" : self.harmony_mean,\n            \"harmony_var\" : self.harmony_var,\n            \"perceptr_mean\" : self.perceptr_mean,\n            \"perceptr_var\": self.perceptr_var,\n            \"tempo\" : self.tempo\n\n        }\n\n        result.update(self.mel_frequencies)\n\n        return  result\n\n\n    def load_audio_file(self):\n        self.audio_file, _ = librosa.effects.trim(self.y)\n\n        return self.audio_file\n\n    def get_harmony(self):\n        harmony = librosa.effects.harmonic(y = self.audio_file)\n        self.harmony_mean = np.mean(harmony)\n        self.harmony_var = np.var(harmony)\n\n        return self.harmony_mean, self.harmony_var\n        \n\n    def get_length(self):\n        self.length = np.shape(self.audio_file)[0]\n        \n        return self.length\n\n    def get_zero_crossings(self):\n        zero_crossings = librosa.zero_crossings(self.audio_file, pad=False)\n\n        self.zero_crossings_rate_mean = np.mean(zero_crossings)\n        self.zero_crossings_rate_var = np.var(zero_crossings)\n\n        return self.zero_crossings_rate_mean, self.zero_crossings_rate_var\n\n    def get_rms(self):\n        rms = librosa.feature.rms(y = self.audio_file)\n\n        self.rms_mean = np.mean(rms)\n        self.rms_var = np.var(rms)\n\n        return self.rms_mean, self.rms_var\n\n    def get_spectral_bandwith(self):\n        spectral_bandwith = librosa.feature.spectral_bandwidth(y = self.audio_file, sr = self.sr)\n        self.spectral_bandwith_mean = np.mean(spectral_bandwith)\n        self.spectral_bandwith_var = np.var(spectral_bandwith)\n\n        return self.spectral_bandwith_mean, self.spectral_bandwith_var\n    \n\n    def get_tempo(self):\n        # Estimate tempo (BPM) from your audio time series\n        self.tempo, _ = librosa.beat.beat_track(\n            y=self.y,\n            sr=self.sr\n        )\n\n        self.tempo = self.tempo[0]\n        \n        return self.tempo\n\n    def get_centroids(self):\n        spectral_centroids = librosa.feature.spectral_centroid(y = self.audio_file, sr=self.sr)[0]\n\n        self.spectral_centriod_mean = np.mean(spectral_centroids)\n        self.spectral_centriod_var = np.var(spectral_centroids)\n\n        return self.spectral_centriod_mean, self.spectral_centriod_var\n\n    def get_perceptr(self):\n        # 1. Compute Mel‐spectrogram\n        S = librosa.feature.melspectrogram(\n            y=self.audio_file,\n            sr=self.sr,\n            hop_length=5000,\n        )\n        \n        pcen = librosa.pcen(\n            S,\n            sr=self.sr,\n            hop_length=5000,\n        )\n        \n        # 3. Compute statistics\n        self.perceptr_mean = np.mean(pcen)\n        self.perceptr_var  = np.var(pcen)\n        \n        \n        return pcen\n    \n    def get_spectral_rolloff(self):\n\n        spectral_rolloff = librosa.feature.spectral_rolloff(y = self.audio_file, sr=self.sr)[0]\n\n        self.rolloff_mean = np.mean(spectral_rolloff)\n        self.rolloff_var = np.var(spectral_rolloff)\n\n\n        return self.rolloff_mean, self.rolloff_var\n\n    def get_chroma(self, hop_length = 5000):\n        \n        chromagram = librosa.feature.chroma_stft(y = self.audio_file, sr=self.sr, hop_length=hop_length)\n        self.chroma_stft_mean = np.mean(chromagram)\n        self.chroma_stft_var = np.var(chromagram)\n\n        return self.chroma_stft_mean , self.chroma_stft_var\n    \n    def get_mel_frequencies(self):\n        mfccs = librosa.feature.mfcc(y = self.audio_file, sr=self.sr)\n        self.mel_frequencies = {}\n        for i, mfcc in enumerate( mfccs):\n            self.mel_frequencies[f'mfcc{i+1}_mean'] = np.mean(mfcc)\n            self.mel_frequencies[f'mfcc{i+1}_var'] = np.var(mfcc)\n\n        return self.mel_frequencies\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T22:17:23.969340Z","iopub.execute_input":"2025-04-27T22:17:23.969628Z","iopub.status.idle":"2025-04-27T22:17:23.984551Z","shell.execute_reply.started":"2025-04-27T22:17:23.969606Z","shell.execute_reply":"2025-04-27T22:17:23.983879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_input_path = '/kaggle/input/musicgenreclassification/genres_original/blues/blues.00000.wav'\n\naudio_features = AudioFeature(test_input_path)\n\ndf = audio_features.get_dataframe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T22:08:08.456509Z","iopub.execute_input":"2025-04-27T22:08:08.456817Z","iopub.status.idle":"2025-04-27T22:08:10.653806Z","shell.execute_reply.started":"2025-04-27T22:08:08.456794Z","shell.execute_reply":"2025-04-27T22:08:10.653201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T22:08:10.654777Z","iopub.execute_input":"2025-04-27T22:08:10.654994Z","iopub.status.idle":"2025-04-27T22:08:10.679095Z","shell.execute_reply.started":"2025-04-27T22:08:10.654979Z","shell.execute_reply":"2025-04-27T22:08:10.678469Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}